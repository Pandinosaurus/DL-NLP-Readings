# Visual Grounding

## Image-based Visual Grounding
### Image Retrieval
- [2019 CVPR] **Deep Supervised Cross-modal Retrieval**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhen_Deep_Supervised_Cross-Modal_Retrieval_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Deep%20Supervised%20Cross-modal%20Retrieval.bib).

### Image Captioning
- [2015 ICML] **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**, [[paper]](https://arxiv.org/pdf/1502.03044.pdf), [[slides]](http://www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Katherine_slides.pdf), [[bibtex]](/Bibtex/Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention.bib),  [[homepage]](http://kelvinxu.github.io/projects/capgen.html), sources: [[kelvinxu/arctic-captions]](https://github.com/kelvinxu/arctic-captions), [[yunjey/show-attend-and-tell]](https://github.com/yunjey/show-attend-and-tell), [[DeepRNN/image_captioning]](https://github.com/DeepRNN/image_captioning), [[coldmanck/show-attend-and-tell]](https://github.com/coldmanck/show-attend-and-tell).
- [2017 PAMI] **Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**, [[paper]](https://arxiv.org/abs/1609.06647.pdf), sources: [[tensorflow/models/im2txt]](https://github.com/tensorflow/models/tree/master/research/im2txt).
- [2018 ACL] **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning**, [[paper]](http://aclweb.org/anthology/P18-1238), [[bibtex]](/Bibtex/Conceptual%20Captions%20-%20A%20Cleaned%20Hypernymed%20Image%20Alt-text%20Dataset%20For%20Automatic%20Image%20Captioning.bib), [[homepage]](https://ai.google.com/research/ConceptualCaptions), sources: [[google-research-datasets/conceptual-captions]](https://github.com/google-research-datasets/conceptual-captions).
- [2018 NeurIPS] **Partially-Supervised Image Captioning**, [[paper]](https://papers.nips.cc/paper/7458-partially-supervised-image-captioning.pdf), [[bibtex]](/Bibtex/Partially-Supervised%20Image%20Captioning.bib).
- [2019 CVPR] **Auto-Encoding Scene Graphs for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.bib), [[post]](https://zhuanlan.zhihu.com/p/41200392).

### Text-based Image Edit
- [2019 ACL] **Expressing Visual Relationships via Language**, [[paper]](https://www.aclweb.org/anthology/P19-1182.pdf), [[bibtex]](/Bibtex/Expressing%20Visual%20Relationships%20via%20Language.bib), sources: [[airsplay/VisualRelationships]](https://github.com/airsplay/VisualRelationships).

### Visual Question Answering
- [2016 NIPS] **Hierarchical Question-Image Co-Attention for Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1606.00061), [[bibtex]](/Bibtex/Hierarchical%20Question-Image%20Co-Attention%20for%20Visual%20Question%20Answering.bib), sources: [[karunraju/VQA]](https://github.com/karunraju/VQA), [[jiasenlu/HieCoAttenVQA]](https://github.com/jiasenlu/HieCoAttenVQA).
- [2018 CVPR] **Visual Grounding via Accumulated Attention**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Visual%20Grounding%20via%20Accumulated%20Attention.bib).
- [2019 ACL] **Multi-grained Attention with Object-level Grounding for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/P19-1349.pdf), [[bibtex]](/Bibtex/Multi-grained%20Attention%20with%20Object-level%20Grounding%20for%20Visual%20Question%20Answering.bib).
- [2019 EMNLP] **LXMERT: Learning Cross-Modality Encoder Representations from Transformers**, [[paper]](https://www.aclweb.org/anthology/D19-1514.pdf), [[bibtex]](/Bibtex/LXMERT%20-%20Learning%20Cross-Modality%20Encoder%20Representations%20from%20Transformers.bib), sources: [[airsplay/lxmert]](https://github.com/airsplay/lxmert).


## Video-based Visual Grounding
### Video Captioning
- [2015 ICCV] **Sequence to Sequence – Video to Text**, [[paper]](http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf), [[bibtex]](/Bibtex/Sequence%20to%20Sequence%20–%20Video%20to%20Text.bib), [[homepage]](https://vsubhashini.github.io/s2vt.html), sources: [[vsubhashini/caffe/examples/s2vt]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt).
- [2017 ICCV] **Dense-Captioning Events in Videos**, [[paper]](https://arxiv.org/pdf/1705.00754.pdf), [[bibtex]](/Bibtex/Dense-Captioning%20Events%20in%20Videos.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/densevid/), source: [[ranjaykrishna/densevid_eval]](https://github.com/ranjaykrishna/densevid_eval).
- [2018 CVPR] **Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning**, [[paper]](https://arxiv.org/pdf/1804.00100.pdf), [[bibtex]](/Bibtex/Bidirectional%20Attentive%20Fusion%20with%20Context%20Gating%20for%20Dense%20Video%20Captioning.bib), sources: [[JaywongWang/DenseVideoCaptioning]](https://github.com/JaywongWang/DenseVideoCaptioning).
- [2018 CVPR] **End-to-End Dense Video Captioning with Masked Transformer**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/End-to-End%20Dense%20Video%20Captioning%20with%20Masked%20Transformer.bib), sources: [[salesforce/densecap]](https://github.com/salesforce/densecap).
- [2018 CVPR] **Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos**, [[paper]](http://vision.stanford.edu/pdf/huang-buch-2018cvpr), [[bibtex]](/Bibtex/Finding%20It%20-%20Weakly-Supervised%20Reference-Aware%20Visual%20Grounding%20in%20Instructional%20Videos.bib), [[supplementary]](https://finding-it.github.io/finding-it-suppmat.pdf), [[poster]](https://drive.google.com/file/d/1uvnw6VDn0r1nS3ePyFKaCbEx5GZw1ZEy/view), [[homepage]](https://finding-it.github.io), [[youtube]](https://www.youtube.com/watch?v=GBo4sFNzhtU&feature=youtu.be&t=1366).
- [2019 WACV] **Joint Event Detection and Description in Continuous Video Streams**, [[paper]](http://www.boyangli.co/paper/huijuanxu-wacv-2019.pdf), [[bibtex]](/Bibtex/Joint%20Event%20Detection%20and%20Description%20in%20Continuous%20Video%20Streams.bib), sources: [[VisionLearningGroup/JEDDi-Net]](https://github.com/VisionLearningGroup/JEDDi-Net).
- [2019 CVPR] **Grounded Video Description**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Grounded%20Video%20Description.bib), sources: [[facebookresearch/ActivityNet-Entities]](https://github.com/facebookresearch/ActivityNet-Entities), [[facebookresearch/grounded-video-description]](https://github.com/facebookresearch/grounded-video-description).
- [2019 CSUR] **Video Description: A Survey of Methods, Datasets, and Evaluation Metrics**, [[paper]](https://arxiv.org/pdf/1806.00186.pdf), [[bibtex]](/Bibtex/Video%20Description%20-%20A%20Survey%20of%20Methods%20Datasets%20and%20Evaluation%20Metrics.bib).
- [2019 ACL] **Dense Procedure Captioning in Narrated Instructional Videos**, [[paper]](https://www.aclweb.org/anthology/P19-1641.pdf), [[bibtex]](/Bibtex/Dense%20Procedure%20Captioning%20in%20Narrated%20Instructional%20Videos.bib).
- [2019 ACL] **Multimodal Abstractive Summarization for How2 Videos**, [[paper]](https://www.aclweb.org/anthology/P19-1659.pdf), [[bibtex]](/Bibtex/Multimodal%20Abstractive%20Summarization%20for%20How2%20Videos.bib).

### Video Clip Localization
- [2017 ICCV] **Localizing Moments in Video with Natural Language**, [[paper]](https://people.eecs.berkeley.edu/~lisa_anne/didemo/paper_arxiv.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Natural%20Language.bib), sources: [[LisaAnne/LocalizingMoments]](https://github.com/LisaAnne/LocalizingMoments).
- [2017 ICCV] **TALL: Temporal Activity Localization via Language Query**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Gao_TALL_Temporal_Activity_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/TALL%20-%20Temporal%20Activity%20Localization%20via%20Language%20Query.bib), sources: [[jiyanggao/TALL]](https://github.com/jiyanggao/TALL).
- [2017 ICCV] **TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals**, [[paper]](https://arxiv.org/pdf/1703.06189.pdf), [[supplementary]](http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Gao_TURN_TAP_Temporal_ICCV_2017_supplemental.pdf), [[bibtex]](/Bibtex/TURN%20TAP%20-%20Temporal%20Unit%20Regression%20Network%20for%20Temporal%20Action%20Proposals.bib), sources: [[jiyanggao/TURN-TAP]](https://github.com/jiyanggao/TURN-TAP).
- [2018 EMNLP] **Localizing Moments in Video with Temporal Language**, [[paper]](https://www.aclweb.org/anthology/D18-1168.pdf), [[bibtex]](/Bibtex/Localizing%20Moments%20in%20Video%20with%20Temporal%20Language.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/D18-1168.Attachment.pdf), sources: [[LisaAnne/TemporalLanguageRelease]](https://github.com/LisaAnne/TemporalLanguageRelease).
- [2018 EMNLP] **Temporally Grounding Natural Sentence in Video**, [[paper]](https://www.aclweb.org/anthology/D18-1015.pdf), [[bibtex]](/Bibtex/Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib).
- [2018 ECCV] **Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bingbin_Liu_Temporal_Modular_Networks_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/Temporal%20Modular%20Networks%20for%20Retrieving%20Complex%20Compositional%20Activities%20in%20Videos.bib), [[homepage]](https://clarabing.github.io/tmn/).
- [2018 ACMMM] **Cross-modal Moment Localization in Videos**, [[paper]](https://liqiangnie.github.io/paper/p843-liu.pdf), [[bibtex]](/Bibtex/Cross-modal%20Moment%20Localization%20in%20Videos.bib).
- [2018 ArXiv] **Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions**, [[paper]](https://arxiv.org/pdf/1808.08803.pdf), [[bibtex]](/Bibtex/Attentive%20Sequence%20to%20Sequence%20Translation%20for%20Localizing%20Clips%20of%20Interest%20by%20Natural%20Language%20Descriptions.bib), sources: [[NeonKrypton/ASST]](https://github.com/NeonKrypton/ASST).
- [2018 SIGIR] **Attentive Moment Retrieval in Videos**, [[paper]](http://staff.ustc.edu.cn/~hexn/papers/sigir18-video-retrieval.pdf), [[bibtex]](/Bibtex/Attentive%20Moment%20Retrieval%20in%20Videos.bib), [[slides]](https://pdfs.semanticscholar.org/5dc8/f69ad9404ed9e8d2318dca19f4eb534440a5.pdf), [[codes]](https://sigir2018.wixsite.com/acrn).
- [2018 AAAI] **To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression**, [[paper]](https://arxiv.org/pdf/1804.07014.pdf), [[bibtex]](/Bibtex/To%20Find%20Where%20You%20Talk%20-%20Temporal%20Sentence%20Localization%20in%20Video%20with%20Attention%20Based%20Location%20Regression.bib).
- [2019 WACV] **MAC: Mining Activity Concepts for Language-based Temporal Localization**, [[paper]](https://arxiv.org/pdf/1811.08925.pdf), [[bibtex]](/Bibtex/MAC%20-%20Mining%20Activity%20Concepts%20for%20Language-based%20Temporal%20Localization.bib), sources: [[runzhouge/MAC]](https://github.com/runzhouge/MAC).
- [2019 NAACL] **ExCL: Extractive Clip Localization Using Natural Language Descriptions**, [[paper]](https://www.aclweb.org/anthology/N19-1198.pdf), [[bibtex]](/Bibtex/ExCL%20-%20Extractive%20Clip%20Localization%20Using%20Natural%20Language%20Descriptions.bib).
- [2019 CVPR] **MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_MAN_Moment_Alignment_Network_for_Natural_Language_Moment_Retrieval_via_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/MAN%20-%20Moment%20Alignment%20Network%20for%20Natural%20Language%20Moment%20Retrieval%20via%20Iterative%20Graph%20Adjustment.bib).
- [2019 CVPR] **Language-driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Language-Driven_Temporal_Activity_Localization_A_Semantic_Matching_Reinforcement_Learning_Model_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Language-driven%20Temporal%20Activity%20Localization%20-%20A%20Semantic%20Matching%20Reinforcement%20Learning%20Model.bib).
- [2019 AAAI] **Localizing Natural Language in Videos**, [[paper]](http://forestlinma.com/welcome_files/Jingyuan_Chen_Localizing_Natural_Language_In_Videos_AAAI_2019.pdf), [[bibtex]](/Bibtex/Localizing%20Natural%20Language%20in%20Videos.bib).
- [2019 AAAI] **Multilevel Language and Vision Integration for Text-to-Clip Retrieval**, [[paper]](https://arxiv.org/pdf/1804.05113.pdf), [[bibtex]](/Bibtex/Multilevel%20Language%20and%20Vision%20Integration%20for%20Text-to-Clip%20Retrieval.bib), sources: [[VisionLearningGroup/Text-to-Clip_Retrieval]](https://github.com/VisionLearningGroup/Text-to-Clip_Retrieval).
- [2019 AAAI] **Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos**, [[paper]](https://arxiv.org/pdf/1901.06829.pdf), [[bibtex]](/Bibtex/Read%20Watch%20and%20Move%20-%20Reinforcement%20Learning%20for%20Temporally%20Grounding%20Natural%20Language%20Descriptions%20in%20Videos.bib).
- [2019 AAAI] **Semantic Proposal for Activity Localization in Videos via Sentence Query**, [[paper]](https://pdfs.semanticscholar.org/8548/d5a93869a5a4c808f5e81742f59f848c718c.pdf?_ga=2.88458585.398432507.1574674952-963912669.1574674952), [[bibtex]](/Bibtex/Semantic%20Proposal%20for%20Activity%20Localization%20in%20Videos%20via%20Sentence%20Query.bib).
- [2019 ArXiv] **Tripping through time: Efficient Localization of Activities in Videos**, [[paper]](https://arxiv.org/pdf/1904.09936.pdf), [[bibtex]](/Bibtex/Tripping%20through%20time%20-%20Efficient%20Localization%20of%20Activities%20in%20Videos.bib).
- [2019 ACL] **Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video**, [[paper]](https://www.aclweb.org/anthology/P19-1183), [[bibtex]](/Bibtex/Weakly-Supervised%20Spatio-Temporally%20Grounding%20Natural%20Sentence%20in%20Video.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/P19-1183.Supplementary.pdf), sources: [[JeffCHEN2017/WSSTG]](https://github.com/JeffCHEN2017/WSSTG).
- [2019 EMNLP] **DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization**, [[paper]](https://www.aclweb.org/anthology/D19-1518.pdf), [[bibtex]](/Bibtex/DEBUG%20-%20A%20Dense%20Bottom-Up%20Grounding%20Approach%20for%20Natural%20Language%20Video%20Localization.bib).

### Visual Question Answering
- [2018 CVPR] **Motion-Appearance Co-Memory Networks for Video Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Motion-Appearance_Co-Memory_Networks_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Motion-Appearance%20Co-Memory%20Networks%20for%20Video%20Question%20Answering.bib).
- [2018 CVPR] **Focal Visual-Text Attention for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Visual%20Question%20Answering.bib), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2018 EMNLP] **TVQA: Localized Compositional Video Question Answering**, [[paper]](https://www.aclweb.org/anthology/D18-1167.pdf), [[bibtex]](/Bibtex/TVQA%20-%20Localized%20Compositional%20Video%20Question%20Answering.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/D18-1167.Attachment.pdf), [[homepage]](http://tvqa.cs.unc.edu), sources: [[jayleicn/TVQA]](https://github.com/jayleicn/TVQA).
- [2019 TPAMI] **Focal Visual-Text Attention for Memex Question Answering**, [[paper]](http://llcao.net/paper/MemexQA_TPAMI.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Memex%20Question%20Answering.bib), [[homepage]](https://memexqa.cs.cmu.edu), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2019 CVPR] **Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Heterogeneous%20Memory%20Enhanced%20Multimodal%20Attention%20Model%20for%20Video%20Question%20Answering.bib), [[poster]](http://homes.sice.indiana.edu/fan6/docs/cvpr19_videoqa.pdf), sources: [[fanchenyou/HME-VideoQA]](https://github.com/fanchenyou/HME-VideoQA).
- [2019 AAAI] **ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering**, [[paper]](https://arxiv.org/pdf/1906.02467.pdf), [[bibtex]](/Bibtex/ActivityNet-QA%20-%20A%20Dataset%20for%20Understanding%20Complex%20Web%20Videos%20via%20Question%20Answering.bib), sources: [[MILVLG/activitynet-qa]](https://github.com/MILVLG/activitynet-qa).
- [2019 ArXiv] **TVQA+: Spatio-Temporal Grounding for Video Question Answering**, [[paper]](https://arxiv.org/pdf/1904.11574.pdf), [[bibtex]](/Bibtex/TVQA+%20-%20Spatio-Temporal%20Grounding%20for%20Video%20Question%20Answering.bib), [[homepage]](http://tvqa.cs.unc.edu).

### Video Grounded Dialogue
- [2019 ACL] **Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems**, [[paper]](https://www.aclweb.org/anthology/P19-1564.pdf), [[bibtex]](/Bibtex/Multimodal%20Transformer%20Networks%20for%20End-to-End%20Video-Grounded%20Dialogue%20Systems.bib), sources: [[henryhungle/MTN]](https://github.com/henryhungle/MTN).


## Others
- [2018 CVPR] **Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Vision-and-Language%20Navigation.bib), sources: [[peteanderson80/Matterport3DSimulator]](https://github.com/peteanderson80/Matterport3DSimulator).
- [2019 ACL] **Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation**, [[paper]](https://www.aclweb.org/anthology/P19-1655.pdf), [[bibtex]](/Bibtex/Are%20You%20Looking%20Grounding%20to%20Multiple%20Modalities%20in%20Vision-and-Language%20Navigation.bib), [[supplementary]](https://www.aclweb.org/anthology/attachments/P19-1655.Supplementary.pdf).
- [2019 ACMMM] **A New Benchmark and Approach for Fine-grained Cross-media Retrieval**, [[paper]](https://arxiv.org/pdf/1907.04476.pdf), [[bibtex]](/Bibtex/A%20New%20Benchmark%20and%20Approach%20for%20Fine-grained%20Cross-media%20Retrieval.bib), [[homepage]](http://59.108.48.34/tiki/FGCrossNet/), sources: [[PKU-ICST-MIPL/FGCrossNet_ACMMM2019]](https://github.com/PKU-ICST-MIPL/FGCrossNet_ACMMM2019).